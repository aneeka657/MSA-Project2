import os
import numpy as np
import librosa
from scipy import signal
import math
import glob
from tqdm import tqdm

# === AUTHOR'S EXACT PARAMETERS ===
audio_sr = 40960  # Author's exact sample rate
window_size = 2048  # Author's FFT window
hop = 1024  # Author's hop length  
n_mels = 80  # Author's mel bands
fmin = 80  # Author's min frequency
fmax = 8000  # Author's max frequency (CRITICAL: not sr/2)
frame_size = 0.5  # Author's final frame size after downsampling
downsample = 20  # Author's downsampling factor
downsample_window = 21  # Author's median filter window

# === FUNCTION DICTIONARY (Author's exact mapping) ===
function_dict = {
    'intro': 0,
    'verse': 1, 
    'chorus': 2,
    'bridge': 3,
    'inst': 4,
    'outro': 5,
    'silence': 6,
}

def func_conversion(label):
    """Convert raw label to standardized function label (author's method)"""
    label = label.lower().strip()
    
    if label in function_dict:
        return label
    
    # ICASSP 2022 mapping (your method)
    substrings = [
        ("silence", "silence"), ("pre-chorus", "verse"), ("prechorus", "verse"),
        ("refrain", "chorus"), ("chorus", "chorus"), ("theme", "chorus"),
        ("stutter", "chorus"), ("verse", "verse"), ("rap", "verse"),
        ("section", "verse"), ("slow", "verse"), ("build", "verse"),
        ("dialog", "verse"), ("intro", "intro"), ("fadein", "intro"),
        ("opening", "intro"), ("bridge", "bridge"), ("trans", "bridge"),
        ("out", "outro"), ("coda", "outro"), ("ending", "outro"),
        ("break", "inst"), ("inst", "inst"), ("interlude", "inst"), 
        ("impro", "inst"), ("solo", "inst")
    ]
    
    for substr, mapped in substrings:
        if substr in label:
            return mapped
    
    return "inst"

# === MISSING AUGMENTATION METHODS (Your 10-file strategy) ===
def create_augmentations():
    """
    Create the missing augmentations list for author's preprocessing.py
    Your strategy: Pre-emphasis {0.7, 0.97} applied to pitch shifts [-2,-1,0,1,2] = 10 files
    """
    augmentations = []
    pitch_shifts = [-2, -1, 0, 1, 2]
    preemph_coeffs = [0.7, 0.97]
    
    for n_steps in pitch_shifts:
        for alpha in preemph_coeffs:
            augmentations.append(f'pitch_{n_steps}_preemph_{alpha:.2f}')
    
    return augmentations

# === MISSING AUDIO_AUGMENTATION FUNCTION ===
def audio_augmentation(y, sr, method='original'):
    """
    The missing audio_augmentation function from author's preprocessing.py
    Applies pitch shift + pre-emphasis as described in the paper
    """
    if method == 'original':
        return y
    
    y_aug = y.copy()
    
    if method.startswith('pitch_') and '_preemph_' in method:
        try:
            # Parse: 'pitch_-2_preemph_0.70' -> n_steps=-2, alpha=0.70
            parts = method.split('_')
            n_steps = int(parts[1])
            alpha = float(parts[3])
            
            # Step 1: Pitch shift (author's exact method)
            if n_steps != 0:
                y_aug = librosa.effects.pitch_shift(y_aug, sr=sr, n_steps=n_steps)
            
            # Step 2: Pre-emphasis (author's exact method)
            if alpha > 0:
                y_preemph = np.zeros_like(y_aug)
                y_preemph[0] = y_aug[0]  # First sample unchanged
                y_preemph[1:] = y_aug[1:] - alpha * y_aug[:-1]  # Pre-emphasis filter
                y_aug = y_preemph
                
        except Exception as e:
            print(f"Error in audio_augmentation: {e}")
            return y
    
    return y_aug

# === AUTHOR'S EXACT FUNCTIONS ===
def median_downsample_feature_sequence(X, filt_len=21, down_sampling=20):
    """Author's exact downsampling method with median filtering"""
    assert filt_len % 2 == 1  # L needs to be odd
    filt_len = [1, filt_len]
    X_smooth = signal.medfilt2d(X, filt_len)
    X_smooth = X_smooth[:, ::down_sampling]
    return X_smooth

def get_functional_labels(annotations, n_frames, frame_size):
    """Author's exact label generation method (made robust)"""
    annotations = annotations.copy()
    
    # Remove annotations beyond frame limit
    valid_mask = np.floor(annotations['onset'] / frame_size) < n_frames
    annotations = annotations[valid_mask]
    
    if len(annotations) == 0:
        # Default to intro if no annotations
        boundary = np.zeros(shape=(n_frames,), dtype=np.int32)
        boundary[0] = 1
        function = np.zeros(shape=(n_frames,), dtype=np.int32)
        section = np.array(['intro'] * n_frames, dtype='U40')
        return boundary, function, section

    onset_in_frames = [math.floor(onset / frame_size) for onset in annotations['onset']]
    
    # Ensure frame 0 is included
    if 0 not in onset_in_frames:
        onset_in_frames.insert(0, 0)
        first_annotation = np.array([(0.0, annotations['section'][0])], 
                                   dtype=[('onset', np.float32), ('section', 'U40')])
        annotations = np.concatenate([first_annotation, annotations])
    
    onset_in_frames = sorted(set(onset_in_frames))
    
    # Create boundary array
    boundary = np.zeros(shape=(n_frames,), dtype=np.int32)
    boundary[onset_in_frames] = 1

    # Create section labels using cumulative sum (author's method)
    boundary_cumsum = np.cumsum(boundary) - 1
    
    # Handle edge cases
    max_boundary_idx = np.max(boundary_cumsum)
    if max_boundary_idx >= len(annotations):
        last_section = annotations['section'][-1]
        padding_needed = max_boundary_idx - len(annotations) + 1
        padding_annotations = np.array([(annotations['onset'][-1], last_section)] * padding_needed,
                                     dtype=[('onset', np.float32), ('section', 'U40')])
        annotations = np.concatenate([annotations, padding_annotations])
    
    section = np.take(annotations['section'], boundary_cumsum)
    function = np.array([function_dict[func_conversion(s)] for s in section])

    return boundary, function, section

def basename(path):
    """Helper function"""
    return os.path.basename(path)

# === CREATE ANNOTATION DICT FROM YOUR MAPPED FILES ===
def read_segment_annotation(annotation_dir, audio_files):
    """
    Create annotation_dict from your mapped annotation files
    Converts your format to author's expected format
    """
    annotation_dict = {}
    
    # Find all .npy annotation files
    annotation_files = glob.glob(os.path.join(annotation_dir, '*.npy'))
    
    for ann_file in annotation_files:
        file_id = os.path.basename(ann_file).replace('.npy', '')
        
        try:
            # Load your mapped annotation data (shape: (n_segments, 2))
            segments = np.load(ann_file, allow_pickle=True)
            
            if len(segments) > 0 and segments.shape[1] >= 2:
                # Convert to author's format
                dtype = [('onset', np.float32), ('section', 'U40')]
                formatted_segments = []
                
                for i in range(len(segments)):
                    time_val = float(segments[i, 0])
                    label_val = str(segments[i, 1])
                    formatted_segments.append((time_val, label_val))
                
                if formatted_segments:
                    formatted_annotations = np.array(formatted_segments, dtype=dtype)
                    # Store in author's expected format: [annotator1, annotator2]
                    annotation_dict[file_id] = [formatted_annotations, None]
                    
        except Exception as e:
            print(f"Error processing annotation {file_id}: {e}")
            continue
    
    print(f"Loaded {len(annotation_dict)} annotations")
    return annotation_dict

# === AUTHOR'S EXACT create_feature_label FUNCTION (COMPLETED) ===
def create_feature_label(audio_files, annotation_dict, save_dir, downsample, downsample_window, max_len=935):
    """
    Author's exact create_feature_label function with all missing pieces filled in
    """
    
    # The missing augmentations list!
    augmentations = create_augmentations()
    print(f'Using {len(augmentations)} augmentation methods:')
    for i, aug in enumerate(augmentations):
        print(f'  {i+1}: {aug}')

    start = 0
    max_duration = 0
    total_processed = 0
    truncated_count = 0
    
    for i_f, file in enumerate(sorted(audio_files)[start:]):
        print(f'\nFile {i_f+start+1}/{len(audio_files)}: {basename(file)}')
        id = basename(file).replace('.wav', '').replace('.mp3', '')
        
        if id not in annotation_dict:
            print(f'âš ï¸ No annotation for {id}')
            continue
            
        multi_annotations = annotation_dict[id]

        # Load audio (author's method)
        try:
            y, sr = librosa.load(file, sr=audio_sr)
            duration = librosa.get_duration(y=y, sr=sr)
            if duration > max_duration:
                max_duration = duration
        except Exception as e:
            print(f'âš ï¸ Error loading {file}: {e}')
            continue

        # Augment audio and get Labels (Author's exact structure!)
        for aug_method in augmentations:
            print(f'  Processing: {aug_method}')
            
            try:
                # The missing audio_augmentation call!
                y_aug = audio_augmentation(y, sr, method=aug_method)

                # Normalize amplitude (author's method)
                y_norm = (y_aug - y_aug.mean()) / y_aug.std()

                # Mel-spectrogram (author's exact parameters)
                spec = librosa.feature.melspectrogram(
                    y=y_norm, sr=sr, n_fft=window_size, hop_length=hop, 
                    n_mels=n_mels, fmin=fmin, fmax=fmax
                )
                spec = median_downsample_feature_sequence(spec, filt_len=downsample_window, down_sampling=downsample)

                # Chromagram (author's exact parameters)
                chromagram = librosa.feature.chroma_cqt(
                    y=y_norm, sr=sr, hop_length=hop, n_chroma=12
                )
                chromagram = median_downsample_feature_sequence(chromagram, filt_len=downsample_window, down_sampling=downsample)

                n_frames = spec.shape[1]
                print(f'    Generated {n_frames} frames')

                # Handle max_len constraint for transformer
                if n_frames > max_len:
                    print(f'    Truncating from {n_frames} to {max_len} frames')
                    spec = spec[:, :max_len]
                    chromagram = chromagram[:, :max_len]
                    n_frames = max_len
                    truncated_count += 1

                # The valid_len is the actual number of frames we're using
                # valid_len = n_frames
                print(f'    spec.shape: {spec.shape}')
                print(f'    chromagram.shape: {chromagram.shape}')
                print(f'    valid_len: {n_frames}')  # ADD THIS LINE

                # Convert annotations to labels (for each annotator)
                for annotator, annotations in enumerate(multi_annotations):
                    if annotations is not None:
                        # Filter annotations to max time
                        max_time = n_frames * frame_size
                        valid_annotations = annotations[annotations['onset'] <= max_time]
                        
                        boundary, function, section = get_functional_labels(
                            valid_annotations, n_frames=n_frames, frame_size=frame_size
                        )

                        print(f'    boundary.shape: {boundary.shape}')
                        print(f'    function.shape: {function.shape}')
                        print(f'    section.shape: {section.shape}')

                        # Check time dimensions (author's check)
                        shape_list = [
                            spec.shape[1],
                            chromagram.shape[1],
                            boundary.shape[0],
                            function.shape[0],
                            section.shape[0]
                        ]
                        assert shape_list.count(shape_list[0]) == len(shape_list), f"Shape mismatch: {shape_list}"

                        # Save processed data (author's exact format)
                        save_key = '_'.join([id, aug_method, 'a' + str(annotator+1)])
                        
                        with open(os.path.join(save_dir, save_key + '_spec.npy'), 'wb') as f:
                            np.save(f, np.transpose(spec, [1, 0]))  # [n_frames, 80]
                        with open(os.path.join(save_dir, save_key + '_chroma.npy'), 'wb') as f:
                            np.save(f, np.transpose(chromagram, [1, 0]))  # [n_frames, 12]
                        with open(os.path.join(save_dir, save_key + '_boundary.npy'), 'wb') as f:
                            np.save(f, boundary)  # [n_frames]
                        with open(os.path.join(save_dir, save_key + '_function.npy'), 'wb') as f:
                            np.save(f, function)  # [n_frames]
                        with open(os.path.join(save_dir, save_key + '_section.npy'), 'wb') as f:
                            np.save(f, section)  # [n_frames]
                        # Additional len file for training compatibility
                        with open(os.path.join(save_dir, save_key + '_len.npy'), 'wb') as f:
                            np.save(f, n_frames)
                        
                        total_processed += 1
                        print(f'    âœ… Saved: {save_key}')
                        
            except Exception as e:
                print(f'    âŒ Error with {aug_method}: {e}')
                continue
    
    print(f'\nProcessing complete!')
    print(f'Max duration: {max_duration:.2f} seconds')
    print(f'Total samples processed: {total_processed}')
    print(f'Samples truncated: {truncated_count}')

# === DATASET CREATION ===
def load_and_combine_processed_files(processed_dir, output_file):
    """Load individual .npy files and create final .npz dataset"""
    
    spec_files = glob.glob(os.path.join(processed_dir, '*_spec.npy'))
    if not spec_files:
        raise ValueError(f"No processed files found in {processed_dir}")
    
    print(f"Found {len(spec_files)} processed samples")
    
    all_data = {
        'spec': [], 'chromagram': [], 'len': [],
        'boundary': [], 'function': [], 'section': []
    }
    
    for spec_file in sorted(spec_files):
        base_name = spec_file.replace('_spec.npy', '')
        
        try:
            # Load all components
            spec = np.load(spec_file)
            chroma = np.load(base_name + '_chroma.npy')
            boundary = np.load(base_name + '_boundary.npy')
            function = np.load(base_name + '_function.npy')
            section = np.load(base_name + '_section.npy')
            
            # Load or compute length
            len_file = base_name + '_len.npy'
            if os.path.exists(len_file):
                valid_len = int(np.load(len_file).item())
            else:
                valid_len = spec.shape[0]
                np.save(len_file, valid_len)
            
            # Store data
            all_data['spec'].append(spec)
            all_data['chromagram'].append(chroma)
            all_data['len'].append(valid_len)
            all_data['boundary'].append(boundary)
            all_data['function'].append(function)
            all_data['section'].append(section)
            
        except Exception as e:
            print(f"Error loading {base_name}: {e}")
            continue
    
    # Convert to numpy arrays
    for key in all_data.keys():
        if key != 'section':
            all_data[key] = np.array(all_data[key], dtype=object)
    
    # Save final dataset
    np.savez_compressed(output_file, **all_data)
    print(f"âœ… Saved final dataset: {output_file}")
    print(f"   Total samples: {len(all_data['spec'])}")
    print(f"   Sequence lengths: min={min(all_data['len'])}, max={max(all_data['len'])}, avg={np.mean(all_data['len']):.1f}")
    
    return all_data

# === MAIN FUNCTION ===
def run_salami_preprocessing(
    target_song_ids=None,
    audio_dir="/Scratch/repository/iahmad/salami_pop",
    annotation_dir="/Scratch/repository/iahmad/salami-mapped-annotation",
    output_dir="./processed_salami_data",
    final_dataset_path="./salami_data/train_data.npz",
    max_len=935
):
    """
    Complete SALAMI preprocessing following author's exact method
    """
    
    print("ðŸŽµ SALAMI Preprocessing - Author's Exact Method + Your Requirements")
    print("=" * 70)
    print(f"ðŸŽ¯ Target songs: {len(target_song_ids) if target_song_ids else 'All'}")
    print(f"ðŸ“ Max sequence length: {max_len} frames ({max_len * frame_size:.1f}s)")
    print(f"ðŸŽ¼ Augmentation: 10x (pitch shifts + pre-emphasis)")
    
    # Create directories
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(os.path.dirname(final_dataset_path), exist_ok=True)
    
    # Step 1: Get audio files
    print(f"\n1ï¸âƒ£ Getting audio files...")
    if target_song_ids:
        audio_files = []
        for song_id in target_song_ids:
            audio_path = os.path.join(audio_dir, f"{song_id}.mp3")
            if os.path.exists(audio_path):
                audio_files.append(audio_path)
        print(f"   Found {len(audio_files)}/{len(target_song_ids)} target audio files")
    else:
        audio_files = glob.glob(os.path.join(audio_dir, "*.mp3"))
        print(f"   Found {len(audio_files)} audio files")
    
    # Step 2: Create annotation dict
    print(f"\n2ï¸âƒ£ Creating annotation dictionary...")
    annotation_dict = read_segment_annotation(annotation_dir, audio_files)
    
    # Step 3: Run preprocessing (author's exact method!)
    print(f"\n3ï¸âƒ£ Running preprocessing...")
    create_feature_label(
        audio_files=audio_files,
        annotation_dict=annotation_dict,
        save_dir=output_dir,
        downsample=downsample,
        downsample_window=downsample_window,
        max_len=max_len
    )
    
    # Step 4: Create final dataset
    print(f"\n4ï¸âƒ£ Creating final dataset...")
    final_dataset = load_and_combine_processed_files(output_dir, final_dataset_path)
    
    print(f"\nðŸŽ‰ Preprocessing complete!")
    print(f"ðŸ“Š Dataset: {final_dataset_path}")
    print(f"ðŸš€ Ready for training!")
    
    return final_dataset_path

# === USAGE ===
if __name__ == "__main__":
    
    # Your specific song IDs
    TARGET_SONG_IDS = [3, 6, 7, 10, 11, 12, 14, 16, 307, 308, 310, 311, 312, 315, 316, 318, 320, 322, 323, 324, 326, 327, 328, 330, 331, 332, 334, 335, 338, 340, 342, 343, 346, 347, 348, 350, 352, 354, 355, 356, 359, 360, 362, 364, 366, 370, 374, 376, 378, 379, 380, 382, 383, 384, 386, 388, 390, 394, 396, 398, 399, 400, 532, 534, 535, 536, 538, 539, 542, 543, 548, 551, 555, 559, 563, 630, 632, 634, 636, 638, 639, 642, 643, 646, 651, 654, 655, 658, 659, 660, 662, 663, 664, 667, 668, 671, 672, 674, 675, 678, 679, 680, 682, 683, 684, 686, 687, 688, 690, 691, 694, 695, 696, 700, 702, 708, 1600, 1602, 1603, 1604, 1607, 1610, 1611, 1614, 1615, 1616, 1618, 1619, 1620, 1622, 1624, 1626, 1627, 1628, 1630, 1631, 1634, 1635, 1640, 1642, 1647, 1648, 1652, 1654]
    
    # Run complete preprocessing
    dataset_path = run_salami_preprocessing(
        target_song_ids=TARGET_SONG_IDS,
        max_len=935
    )
    
    print(f"\nâœ… Final dataset ready: {dataset_path}")
    print("ðŸŽ¯ Expected performance improvement: F1_seg 0.428 â†’ 0.50-0.57")